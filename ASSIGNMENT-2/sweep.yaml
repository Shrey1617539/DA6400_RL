# sweep.yaml

method: bayes
command: ["python", "main.py"]
metric:
  goal: minimize
  name: cumulative_mean_regret
parameters:
  algo:
    values: ["DDQN", "MC_REINFORCE"]
  
  # Environment settings
  env_name:
    values: ["CartPole-v1", "Acrobot-v1"]
  episodes:
    value: 1000
  experiments:
    value: 3  # Reduced from 5 to save computation time during sweep
  
  # Learning parameters
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-2
  learning_rate1:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-2
  learning_rate2:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-2
  
  # Policy parameters (for DDQN)
  policy:
    values: ["epsilon_greedy", "softmax"]
  param:
    distribution: uniform
    min: 0.1
    max: 100
  param_decay:
    distribution: uniform
    min: 0.9
    max: 0.999
  final_param:
    distribution: uniform
    min: 0.01
    max: 0.2
  
  # Network architecture
  fc_units_lis:
    values: [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]]
  Type:
    values: ["Type 1", "Type 2"]
  buffer_size:
    values: [10000, 100000, 1000000]
  batch_size:
    values: [32, 64, 128, 256]
  update_every:
    values: [4, 10, 100, 200]
  
  # MC REINFORCE specific
  baseline:
    values: [true, false]